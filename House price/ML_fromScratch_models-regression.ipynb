{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class StandardScaler2:\n",
    "    def __init__(self):\n",
    "        self.mean_ = None\n",
    "        self.scale_ = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Compute the mean and standard deviation for scaling.\n",
    "        \"\"\"\n",
    "        self.mean_ = np.mean(X, axis=0)  # Mean of each feature\n",
    "        self.scale_ = np.std(X, axis=0)  # Standard deviation of each feature\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Perform standardization by centering and scaling.\n",
    "        \"\"\"\n",
    "        if self.mean_ is None or self.scale_ is None:\n",
    "            raise ValueError(\"The scaler has not been fitted yet. Call `fit` first.\")\n",
    "        return (X - self.mean_) / self.scale_\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        Fit to data, then transform it.\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the decision tree regressor.\n",
    "        \"\"\"\n",
    "        self.tree = self._build_tree(X, y)\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        \"\"\"\n",
    "        Recursively build the decision tree.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        if n_samples == 0 or (self.max_depth and depth >= self.max_depth):\n",
    "            return np.mean(y)\n",
    "\n",
    "        best_split = self._find_best_split(X, y)\n",
    "        if best_split is None:\n",
    "            return np.mean(y)\n",
    "\n",
    "        left_indices = X.iloc[:, best_split['feature']] < best_split['threshold']\n",
    "        right_indices = X.iloc[:, best_split['feature']] >= best_split['threshold']\n",
    "\n",
    "        left_tree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_tree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        return {\n",
    "            'feature': best_split['feature'],\n",
    "            'threshold': best_split['threshold'],\n",
    "            'left': left_tree,\n",
    "            'right': right_tree\n",
    "        }\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Find the best feature and threshold to split the data.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        best_mse = float('inf')\n",
    "        best_split = None\n",
    "\n",
    "        for feature in range(n_features):\n",
    "            thresholds = np.unique(X.iloc[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                left_indices = X.iloc[:, feature] < threshold\n",
    "                right_indices = X.iloc[:, feature] >= threshold\n",
    "\n",
    "                if len(y[left_indices]) == 0 or len(y[right_indices]) == 0:\n",
    "                    continue\n",
    "\n",
    "                mse = self._calculate_mse(y, left_indices, right_indices)\n",
    "                if mse < best_mse:\n",
    "                    best_mse = mse\n",
    "                    best_split = {'feature': feature, 'threshold': threshold}\n",
    "\n",
    "        return best_split\n",
    "\n",
    "    def _calculate_mse(self, y, left_indices, right_indices):\n",
    "        \"\"\"\n",
    "        Calculate the mean squared error for a split.\n",
    "        \"\"\"\n",
    "        left_mean = np.mean(y[left_indices])\n",
    "        right_mean = np.mean(y[right_indices])\n",
    "        left_mse = np.mean((y[left_indices] - left_mean) ** 2)\n",
    "        right_mse = np.mean((y[right_indices] - right_mean) ** 2)\n",
    "        return left_mse * len(y[left_indices]) + right_mse * len(y[right_indices])\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict target values for samples in X.\n",
    "        \"\"\"\n",
    "        if self.tree is None:\n",
    "            raise ValueError(\"Model is not trained. Call `fit` first.\")\n",
    "\n",
    "        # Ensure X is a DataFrame or convert it to a suitable format\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            # Convert each row of the DataFrame to a numpy array\n",
    "            return np.array([self._predict_sample(row, self.tree) for row in X.to_numpy()])\n",
    "        else:\n",
    "            return np.array([self._predict_sample(sample, self.tree) for sample in X])\n",
    "\n",
    "    def _predict_sample(self, sample, node):\n",
    "        \"\"\"\n",
    "        Predict the target value for a single sample.\n",
    "        \"\"\"\n",
    "        if isinstance(node, dict):\n",
    "            feature_index = node['feature']\n",
    "            if sample[feature_index] < node['threshold']:\n",
    "                return self._predict_sample(sample, node['left'])\n",
    "            else:\n",
    "                return self._predict_sample(sample, node['right'])\n",
    "        return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_2(X, y, test_size=0.2, random_state=None):\n",
    "    import numpy as np\n",
    "\n",
    "    # Set random state for reproducibility\n",
    "    if random_state:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    # Generate shuffled indices\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Split indices into train and test sets\n",
    "    test_size = int(len(X) * test_size)\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "\n",
    "    # Split the data using slicing\n",
    "    X_train, X_test = X.iloc[train_indices], X.iloc[test_indices]\n",
    "    y_train, y_test = y.iloc[train_indices], y.iloc[test_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd# Load data\n",
    "data = pd.read_csv(\"house.csv\")\n",
    "\n",
    "# Prepare features and target\n",
    "X = data.drop(columns=['date', 'price', 'country'])\n",
    "y = data['price']\n",
    "\n",
    "# Encode categorical columns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col])\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "# Scale features\n",
    "sc = StandardScaler2()\n",
    "X = pd.DataFrame(sc.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split_2(X, y, random_state=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "DT_reg = DecisionTreeRegressor(max_depth=2)\n",
    "DT_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "ypred = DT_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "r2 = r2_score(y_test, ypred)\n",
    "mae = mean_absolute_error(y_test, ypred)\n",
    "mse = mean_squared_error(y_test, ypred)\n",
    "\n",
    "print(\"R-squared Score:\", r2)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=10):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for i in range(self.n_iterations):\n",
    "            y_pred = np.dot(X, self.weights) + self.bias\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "            # Print progress (optional)\n",
    "            if i % 100 == 0:\n",
    "                mse = np.mean((y_pred - y) ** 2)\n",
    "                print(f\"Iteration {i}, MSE: {mse}, Weights: {self.weights}, Bias: {self.bias}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.weights is None or self.bias is None:\n",
    "            raise ValueError(\"Model is not trained. Call `fit` first.\")\n",
    "        return np.dot(X, self.weights) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_LR=LR.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "r2_LR = r2_score(y_test, ypred_LR)\n",
    "mae_LR = mean_absolute_error(y_test, ypred_LR)\n",
    "mse_LR= mean_squared_error(y_test, ypred_LR)\n",
    "\n",
    "print(\"R-squared Score:\", r2_LR)\n",
    "print(\"Mean Absolute Error:\", mae_LR)\n",
    "print(\"Mean Squared Error:\", mse_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class KNNRegressor:\n",
    "    def __init__(self, k=5):  # Corrected __init__ method\n",
    "        self.k = k\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Store the training data.\n",
    "        \"\"\"\n",
    "        # Ensure the input training data is numeric\n",
    "        if isinstance(X_train, pd.DataFrame):\n",
    "            # Check if all columns in the DataFrame are numeric\n",
    "            if not all([np.issubdtype(dtype, np.number) for dtype in X_train.dtypes]):\n",
    "                raise ValueError(\"Training data contains non-numeric values.\")\n",
    "            # Convert DataFrame to NumPy array for processing\n",
    "            X_train = X_train.values\n",
    "        elif isinstance(X_train, np.ndarray):\n",
    "            # Ensure the NumPy array is numeric\n",
    "            if not np.issubdtype(X_train.dtype, np.number):\n",
    "                raise ValueError(\"Training data contains non-numeric values.\")\n",
    "        else:\n",
    "            raise TypeError(\"X_train must be either a Pandas DataFrame or a NumPy array.\")\n",
    "\n",
    "        # Ensure the target data is numeric\n",
    "        if isinstance(y_train, pd.Series):\n",
    "            if not np.issubdtype(y_train.dtype, np.number):\n",
    "                raise ValueError(\"Target data contains non-numeric values.\")\n",
    "            y_train = y_train.values\n",
    "        elif isinstance(y_train, np.ndarray):\n",
    "            if not np.issubdtype(y_train.dtype, np.number):\n",
    "                raise ValueError(\"Target data contains non-numeric values.\")\n",
    "        else:\n",
    "            raise TypeError(\"y_train must be either a Pandas Series or a NumPy array.\")\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Predict the target values for the test data.\n",
    "        \"\"\"\n",
    "        # Check if X_test is a Pandas DataFrame\n",
    "        if isinstance(X_test, pd.DataFrame):\n",
    "            # Ensure all columns in the DataFrame are numeric\n",
    "            if not all([np.issubdtype(dtype, np.number) for dtype in X_test.dtypes]):\n",
    "                raise ValueError(\"Test data contains non-numeric values.\")\n",
    "            # Convert DataFrame to NumPy array for processing\n",
    "            X_test = X_test.values\n",
    "        elif isinstance(X_test, np.ndarray):\n",
    "            # Ensure the NumPy array is numeric\n",
    "            if not np.issubdtype(X_test.dtype, np.number):\n",
    "                raise ValueError(\"Test data contains non-numeric values.\")\n",
    "        else:\n",
    "            raise TypeError(\"X_test must be either a Pandas DataFrame or a NumPy array.\")\n",
    "\n",
    "        # Predict using the processed X_test\n",
    "        y_pred = [self._predict(x) for x in X_test]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict the target value for a single query point.\n",
    "        \"\"\"\n",
    "        # Calculate distances between the query point and all training points\n",
    "        distances = [self._euclidean_distance(x, x_train) for x_train in self.X_train]\n",
    "\n",
    "        # Get the indices of the k nearest neighbors\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "\n",
    "        # Get the target values of the k nearest neighbors\n",
    "        k_nearest_targets = [self.y_train[i] for i in k_indices]\n",
    "\n",
    "        # Return the average of the k nearest neighbors' target values\n",
    "        return np.mean(k_nearest_targets)\n",
    "\n",
    "    def _euclidean_distance(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Calculate the Euclidean distance between two points.\n",
    "        \"\"\"\n",
    "        return np.sqrt(np.sum((x1 - x2) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn=KNNRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_knn=knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "r2_knn = r2_score(y_test, ypred_knn)\n",
    "mae_knn = mean_absolute_error(y_test, ypred_knn)\n",
    "mse_knn= mean_squared_error(y_test, ypred_knn)\n",
    "\n",
    "print(\"R-squared Score:\", r2_knn)\n",
    "print(\"Mean Absolute Error:\", mae_knn)\n",
    "print(\"Mean Squared Error:\", mse_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "\n",
    "class GaussianNaiveBayesRegressor:\n",
    "    def __init__(self, epsilon=1e-9, min_prob=1e-9):\n",
    "        self.mean_ = None  # Mean of each feature for each target value\n",
    "        self.var_ = None   # Variance of each feature for each target value\n",
    "        self.target_values_ = None  # Unique target values\n",
    "        self.epsilon = epsilon  # Small constant to avoid division by zero\n",
    "        self.min_prob = min_prob  # Minimum probability to avoid log(0)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Gaussian Naive Bayes model to the training data.\n",
    "        \"\"\"\n",
    "        # Convert inputs to NumPy arrays if they are pandas DataFrames/Series\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.values\n",
    "\n",
    "        # Get unique target values\n",
    "        self.target_values_ = np.unique(y)\n",
    "\n",
    "        # Initialize arrays to store mean and variance for each feature and target value\n",
    "        n_targets = len(self.target_values_)\n",
    "        n_features = X.shape[1]\n",
    "        self.mean_ = np.zeros((n_targets, n_features))\n",
    "        self.var_ = np.zeros((n_targets, n_features))\n",
    "\n",
    "        # Calculate mean and variance for each target value\n",
    "        for i, target in enumerate(self.target_values_):\n",
    "            X_target = X[y == target]  # Filter data for the current target value\n",
    "            self.mean_[i, :] = X_target.mean(axis=0)  # Mean of each feature\n",
    "            self.var_[i, :] = X_target.var(axis=0)    # Variance of each feature\n",
    "\n",
    "            # Add epsilon to avoid zero variance\n",
    "            self.var_[i, :] += self.epsilon\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the target values for the test data.\n",
    "        \"\"\"\n",
    "        # Convert input to NumPy array if it is a pandas DataFrame\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "\n",
    "        # Initialize array to store predictions\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "\n",
    "        # Predict for each sample in X\n",
    "        for i, x in enumerate(X):\n",
    "            likelihoods = []\n",
    "\n",
    "            # Calculate likelihood for each target value\n",
    "            for j, target in enumerate(self.target_values_):\n",
    "                # Calculate Gaussian PDF and clip values to avoid log(0)\n",
    "                pdf_values = norm.pdf(x, self.mean_[j, :], np.sqrt(self.var_[j, :]))\n",
    "                pdf_values = np.clip(pdf_values, self.min_prob, None)  # Clip to avoid extremely small values\n",
    "                likelihood = np.sum(np.log(pdf_values))  # Log likelihood\n",
    "                likelihoods.append(likelihood)\n",
    "\n",
    "            # Assign the target value with the highest likelihood\n",
    "            y_pred[i] = self.target_values_[np.argmax(likelihoods)]\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB=GaussianNaiveBayesRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_NB=NB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "r2_NB = r2_score(y_test, ypred_NB)\n",
    "mae_NB = mean_absolute_error(y_test, ypred_NB)\n",
    "mse_NB= mean_squared_error(y_test, ypred_NB)\n",
    "\n",
    "print(\"R-squared Score:\", r2_NB)\n",
    "print(\"Mean Absolute Error:\", mae_NB)\n",
    "print(\"Mean Squared Error:\", mse_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SVMRegressor:\n",
    "    def __init__(self, learning_rate=0.01, lambda_param=0.01, epsilon=0.1, n_iters=1000):\n",
    "        \"\"\"\n",
    "        Initialize the SVM Regressor.\n",
    "\n",
    "        Parameters:\n",
    "        - learning_rate: Step size for gradient descent.\n",
    "        - lambda_param: Regularization parameter.\n",
    "        - epsilon: Threshold for epsilon-insensitive loss.\n",
    "        - n_iters: Number of iterations for training.\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.epsilon = epsilon\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the SVM Regressor using Stochastic Gradient Descent (SGD).\n",
    "\n",
    "        Parameters:\n",
    "        - X: Training features (n_samples, n_features). Can be a NumPy array or pandas DataFrame.\n",
    "        - y: Target values (n_samples,). Can be a NumPy array or pandas Series.\n",
    "        \"\"\"\n",
    "        # Convert inputs to NumPy arrays if they are pandas DataFrames/Series\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.values\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Initialize weights and bias\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # Stochastic Gradient Descent\n",
    "        for _ in range(self.n_iters):\n",
    "            for i in range(n_samples):\n",
    "                # Compute the predicted value\n",
    "                y_pred = np.dot(X[i], self.weights) + self.bias\n",
    "\n",
    "                # Compute the error\n",
    "                error = y[i] - y_pred\n",
    "\n",
    "                # Update weights and bias based on the epsilon-insensitive loss\n",
    "                if abs(error) <= self.epsilon:\n",
    "                    self.weights -= self.learning_rate * (2 * self.lambda_param * self.weights)\n",
    "                else:\n",
    "                    self.weights -= self.learning_rate * (\n",
    "                        2 * self.lambda_param * self.weights - np.sign(error) * X[i]\n",
    "                    )\n",
    "                    self.bias -= self.learning_rate * np.sign(error)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the target values for the test data.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Test features (n_samples, n_features). Can be a NumPy array or pandas DataFrame.\n",
    "\n",
    "        Returns:\n",
    "        - Predicted values (n_samples,).\n",
    "        \"\"\"\n",
    "        # Convert input to NumPy array if it is a pandas DataFrame\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "\n",
    "        return np.dot(X, self.weights) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm=SVMRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_svm=svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "r2_svm = r2_score(y_test, ypred_svm)\n",
    "mae_svm = mean_absolute_error(y_test, ypred_svm)\n",
    "mse_svm= mean_squared_error(y_test, ypred_svm)\n",
    "\n",
    "print(\"R-squared Score:\", r2_svm)\n",
    "print(\"Mean Absolute Error:\", mae_svm)\n",
    "print(\"Mean Squared Error:\", mse_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
